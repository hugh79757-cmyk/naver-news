import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def fetch_naver_ranking_news():
    """
    [ë„¤ì´ë²„] ê° ì–¸ë¡ ì‚¬ë³„ ë­í‚¹ 1ìœ„ ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    print("ğŸ•·ï¸ [Naver] ì–¸ë¡ ì‚¬ë³„ 1ìœ„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    url = "https://news.naver.com/main/ranking/popularDay.naver"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
    
    try:
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ì–¸ë¡ ì‚¬ë³„ ë°•ìŠ¤ë“¤
        press_boxes = soup.select('.rankingnews_box')
        
        news_list = []
        for box in press_boxes:
            # 1. ì–¸ë¡ ì‚¬ ì´ë¦„
            press_name = box.select_one('.rankingnews_name')
            if press_name:
                press_name = press_name.get_text(strip=True)
            else:
                continue

            # 2. 1ìœ„ ë‰´ìŠ¤ (ì²« ë²ˆì§¸ ë§í¬)
            first_news = box.select_one('.list_content > li > a')
            if first_news:
                title = first_news.get_text(strip=True)
                # ì¶œì²˜ í‘œê¸°: [ì¡°ì„ ì¼ë³´] ê¸°ì‚¬ì œëª©
                news_list.append(f"[{press_name}] {title}")

        # ë„ˆë¬´ ë§ìœ¼ë©´ ë¹„ìš© ë‚˜ê°€ë‹ˆê¹Œ 20ê°œë§Œ (ì œëª© ê¸´ ìˆœì„œë¡œ ì •ë ¬í•´ì„œ ì•Œì°¬ ê²ƒë§Œ)
        news_list.sort(key=len, reverse=True)
        final_list = news_list[:20]
        
        print(f"âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ {len(final_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return final_list

    except Exception as e:
        print(f"âŒ [Naver] ìˆ˜ì§‘ ì—ëŸ¬: {e}")
        return []

def fetch_policy_api():
    """ 
    [ì •ì±…ë¸Œë¦¬í•‘] ê³µê³µë°ì´í„° APIë¡œ ì •ë¶€ ì •ì±… ë‰´ìŠ¤ ìˆ˜ì§‘ 
    """
    print("ğŸ›ï¸ [Policy] ì •ì±…ë¸Œë¦¬í•‘ API ìš”ì²­ ì¤‘...")
    
    api_key = os.environ.get("DATA_GO_KR_KEY")
    if not api_key:
        print("âš ï¸ ê³µê³µë°ì´í„° API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. (.env í™•ì¸)")
        return []

    url = "http://apis.data.go.kr/1371000/policyNewsService/getPolicyNewsList"
    params = {
        "serviceKey": api_key,
        "numOfRows": 10,
        "pageNo": 1
    }
    
    try:
        res = requests.get(url, params=params)
        soup = BeautifulSoup(res.content, 'xml') # XML íŒŒì‹±
        
        items = soup.find_all('item')
        policy_list = []
        
        # ëˆ ë˜ëŠ” í‚¤ì›Œë“œ í•„í„°ë§
        money_keywords = ["ì§€ì›", "ì‹ ì²­", "ì§€ê¸‰", "í™˜ê¸‰", "ë¬´ë£Œ", "ê°œì‹œ", "íŠ¹ê°€"]
        
        for item in items:
            title = item.find('title').text
            # í‚¤ì›Œë“œê°€ ìˆê±°ë‚˜, ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë‹¤ ê°€ì ¸ì˜¤ê¸° (ì •ì±…ì€ ë‹¤ ì¢‹ìœ¼ë‹ˆê¹Œ)
            if any(k in title for k in money_keywords):
                policy_list.append(f"[ì •ë¶€ì •ì±…] {title}")
            else:
                # í‚¤ì›Œë“œ ì—†ì–´ë„ ìµœê·¼ 3ê°œëŠ” ë¬´ì¡°ê±´ í¬í•¨
                if len(policy_list) < 3:
                    policy_list.append(f"[ì •ë¶€ì •ì±…] {title}")
                    
        print(f"âœ… ì •ì±… ë‰´ìŠ¤ {len(policy_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return policy_list

    except Exception as e:
        print(f"âŒ [Policy] API ì—ëŸ¬: {e}")
        return []
