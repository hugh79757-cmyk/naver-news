import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import time

load_dotenv()

def fetch_naver_ranking_news():
    """ë„¤ì´ë²„ ì–¸ë¡ ì‚¬ë³„ ë­í‚¹ 1ìœ„ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    print("    ğŸ•·ï¸  [Naver] í¬ë¡¤ë§ ì‹œì‘...")
    url = "https://news.naver.com/main/ranking/popularDay.naver"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Referer": "https://www.naver.com/",
        "Accept-Language": "ko-KR,ko;q=0.9"
    }
    
    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        
        press_boxes = soup.select('.rankingnews_box')
        if not press_boxes:
            print("    âš ï¸  [Naver] ë‰´ìŠ¤ ë°•ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        news_list = []
        for box in press_boxes:
            press_name = box.select_one('.rankingnews_name')
            if not press_name:
                continue
            press_name = press_name.get_text(strip=True)

            first_news = box.select_one('.list_content > li > a')
            if first_news:
                title = first_news.get_text(strip=True)
                news_list.append(f"[{press_name}] {title}")

        # ì œëª© ê¸¸ì´ìˆœ ì •ë ¬ í›„ ìƒìœ„ 20ê°œ
        news_list.sort(key=len, reverse=True)
        final_list = news_list[:20]
        
        return final_list

    except requests.RequestException as e:
        print(f"    âŒ [Naver] ìš”ì²­ ì—ëŸ¬: {e}")
        return []
    except Exception as e:
        print(f"    âŒ [Naver] íŒŒì‹± ì—ëŸ¬: {e}")
        return []


def fetch_policy_api():
    """ì •ì±…ë¸Œë¦¬í•‘ API ë‰´ìŠ¤ ìˆ˜ì§‘"""
    print("    ğŸ›ï¸  [Policy] API ìš”ì²­ ì¤‘...")
    
    api_key = os.environ.get("DATA_GO_KR_KEY")
    if not api_key:
        print("    âš ï¸  [Policy] API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    url = "http://apis.data.go.kr/1371000/policyNewsService/getPolicyNewsList"
    params = {
        "serviceKey": api_key,
        "numOfRows": 10,
        "pageNo": 1
    }
    
    try:
        res = requests.get(url, params=params, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.content, 'xml')
        
        items = soup.find_all('item')
        if not items:
            print("    âš ï¸  [Policy] ë‰´ìŠ¤ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        policy_list = []
        money_keywords = ["ì§€ì›", "ì‹ ì²­", "ì§€ê¸‰", "í™˜ê¸‰", "ë¬´ë£Œ", "ê°œì‹œ", "íŠ¹ê°€", "í˜œíƒ"]
        
        for item in items:
            title_tag = item.find('title')
            if not title_tag:
                continue
            title = title_tag.text
            
            # í‚¤ì›Œë“œ í•„í„°ë§ ë˜ëŠ” ìƒìœ„ 3ê°œ ë¬´ì¡°ê±´ í¬í•¨
            if any(k in title for k in money_keywords) or len(policy_list) < 3:
                policy_list.append(f"[ì •ë¶€ì •ì±…] {title}")
                
        return policy_list

    except requests.RequestException as e:
        print(f"    âŒ [Policy] API ì—ëŸ¬: {e}")
        return []
    except Exception as e:
        print(f"    âŒ [Policy] íŒŒì‹± ì—ëŸ¬: {e}")
        return []


def fetch_daum_news():
    """ë‹¤ìŒ ë‰´ìŠ¤ ë­í‚¹ ìˆ˜ì§‘"""
    print("    ğŸ•·ï¸  [Daum] í¬ë¡¤ë§ ì‹œì‘...")
    url = "https://news.daum.net/ranking/popular"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        
        news_list = soup.select('.list_news2 .link_txt')
        if not news_list:
            print("    âš ï¸  [Daum] ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        headlines = []
        for news in news_list[:15]:
            title = news.get_text(strip=True)
            if title:
                headlines.append(f"[Daum] {title}")
                
        return headlines
        
    except requests.RequestException as e:
        print(f"    âŒ [Daum] ìš”ì²­ ì—ëŸ¬: {e}")
        return []
    except Exception as e:
        print(f"    âŒ [Daum] íŒŒì‹± ì—ëŸ¬: {e}")
        return []
