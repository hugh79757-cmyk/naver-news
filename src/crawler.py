import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import time

load_dotenv()

def fetch_naver_ranking_news():
    """ë„¤ì´ë²„ ì–¸ë¡ ì‚¬ë³„ ë­í‚¹ ë‰´ìŠ¤ ìˆ˜ì§‘ (2026ë…„ 1ì›” ìµœì‹  ë²„ì „)"""
    print("    ğŸ•·ï¸  [Naver] í¬ë¡¤ë§ ì‹œì‘...")
    url = "https://news.naver.com/main/ranking/popularDay.naver"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.naver.com/",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"
    }
    
    try:
        res = requests.get(url, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ë°©ë²• 1: ë­í‚¹ ë‰´ìŠ¤ ë°•ìŠ¤
        press_boxes = soup.select('.rankingnews_box')
        
        if not press_boxes:
            # ë°©ë²• 2: ëŒ€ì²´ ì…€ë ‰í„° ì‹œë„
            print("    ğŸ”„ [Naver] ëŒ€ì²´ ì…€ë ‰í„° ì‹œë„...")
            press_boxes = soup.select('div.rankingnews_box_wrap div.rankingnews_box')
        
        if not press_boxes:
            print("    âš ï¸  [Naver] ë‰´ìŠ¤ ë°•ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"    ğŸ’¡ HTML ì¼ë¶€: {soup.text[:200]}")
            return []
        
        news_list = []
        for box in press_boxes:
            # ì–¸ë¡ ì‚¬ ì´ë¦„
            press_name = box.select_one('.rankingnews_name')
            if not press_name:
                press_name = box.select_one('strong.rankingnews_name')
            if not press_name:
                continue
            press_name = press_name.get_text(strip=True)

            # 1ìœ„ ë‰´ìŠ¤
            first_news = box.select_one('.list_content li a')
            if not first_news:
                first_news = box.select_one('ul.rankingnews_list li a')
            
            if first_news:
                title = first_news.get_text(strip=True)
                news_list.append(f"[{press_name}] {title}")

        if news_list:
            # ì œëª© ê¸¸ì´ìˆœ ì •ë ¬ í›„ ìƒìœ„ 20ê°œ
            news_list.sort(key=len, reverse=True)
            final_list = news_list[:20]
            print(f"    âœ… {len(final_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            return final_list
        else:
            print("    âš ï¸  [Naver] ë‰´ìŠ¤ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

    except requests.RequestException as e:
        print(f"    âŒ [Naver] ìš”ì²­ ì—ëŸ¬: {e}")
        return []
    except Exception as e:
        print(f"    âŒ [Naver] íŒŒì‹± ì—ëŸ¬: {e}")
        return []


def fetch_policy_api():
    """ì •ì±…ë¸Œë¦¬í•‘ RSS ë°©ì‹ìœ¼ë¡œ ìˆ˜ì§‘ (API ëŒ€ì‹ )"""
    print("    ğŸ›ï¸  [Policy] RSS ìˆ˜ì§‘ ì¤‘...")
    
    # API ëŒ€ì‹  RSS ì‚¬ìš©
    url = "https://www.korea.kr/rss/policy.xml"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }
    
    try:
        res = requests.get(url, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.content, 'xml')
        
        items = soup.find_all('item')
        if not items:
            print("    âš ï¸  [Policy] RSS í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        policy_list = []
        money_keywords = ["ì§€ì›", "ì‹ ì²­", "ì§€ê¸‰", "í™˜ê¸‰", "ë¬´ë£Œ", "ê°œì‹œ", "íŠ¹ê°€", "í˜œíƒ", "ë³´ì¡°ê¸ˆ"]
        
        for item in items[:15]:  # ìƒìœ„ 15ê°œ
            title_tag = item.find('title')
            if not title_tag:
                continue
            title = title_tag.text.strip()
            
            # í‚¤ì›Œë“œ í•„í„°ë§
            if any(k in title for k in money_keywords):
                policy_list.append(f"[ì •ë¶€ì •ì±…] {title}")
                
        if policy_list:
            print(f"    âœ… {len(policy_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            return policy_list
        else:
            print("    âš ï¸  [Policy] í‚¤ì›Œë“œ ë§¤ì¹­ ë‰´ìŠ¤ ì—†ìŒ")
            return []

    except requests.RequestException as e:
        print(f"    âŒ [Policy] RSS ì—ëŸ¬: {e}")
        return []
    except Exception as e:
        print(f"    âŒ [Policy] íŒŒì‹± ì—ëŸ¬: {e}")
        return []


def fetch_daum_news():
    """ë‹¤ìŒ ë‰´ìŠ¤ ë­í‚¹ ìˆ˜ì§‘ (2026ë…„ 1ì›” ìµœì‹  URL)"""
    print("    ğŸ•·ï¸  [Daum] í¬ë¡¤ë§ ì‹œì‘...")
    url = "https://news.daum.net/ranking/popular/"  # URL ëì— / ì¶”ê°€
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9"
    }
    
    try:
        res = requests.get(url, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ë°©ë²• 1: ê¸°ì¡´ ì…€ë ‰í„°
        news_list = soup.select('.list_news2 .link_txt')
        
        if not news_list:
            # ë°©ë²• 2: ëŒ€ì²´ ì…€ë ‰í„°
            print("    ğŸ”„ [Daum] ëŒ€ì²´ ì…€ë ‰í„° ì‹œë„...")
            news_list = soup.select('ul.list_news2 li a.link_txt')
        
        if not news_list:
            # ë°©ë²• 3: ë” ë„“ì€ ë²”ìœ„
            news_list = soup.select('div.rank_news a')
        
        if not news_list:
            print("    âš ï¸  [Daum] ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"    ğŸ’¡ HTML ì¼ë¶€: {soup.text[:200]}")
            return []
        
        headlines = []
        for news in news_list[:15]:
            title = news.get_text(strip=True)
            if title and len(title) > 10:  # ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸
                headlines.append(f"[Daum] {title}")
        
        if headlines:
            print(f"    âœ… {len(headlines)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            return headlines
        else:
            print("    âš ï¸  [Daum] ìœ íš¨í•œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
    except requests.RequestException as e:
        print(f"    âŒ [Daum] ìš”ì²­ ì—ëŸ¬: {e}")
        return []
    except Exception as e:
        print(f"    âŒ [Daum] íŒŒì‹± ì—ëŸ¬: {e}")
        return []


# ì¶”ê°€: ë„¤ì´ë²„ ë©”ì¸ í—¤ë“œë¼ì¸ ìˆ˜ì§‘ (ë°±ì—…ìš©)
def fetch_naver_main_headlines():
    """ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ í—¤ë“œë¼ì¸ ìˆ˜ì§‘ (ë°±ì—…ìš©)"""
    print("    ğŸ•·ï¸  [Naver Main] í¬ë¡¤ë§ ì‹œì‘...")
    url = "https://news.naver.com/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }
    
    try:
        res = requests.get(url, headers=headers, timeout=15)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # í—¤ë“œë¼ì¸ ìˆ˜ì§‘
        headlines = soup.select('.cjs_news_headlines .cjs_t')
        
        if not headlines:
            headlines = soup.select('.sh_text._sh_text_headline')
        
        news_list = []
        for h in headlines[:20]:
            title = h.get_text(strip=True)
            if title and len(title) > 15:
                news_list.append(f"[ë„¤ì´ë²„ë©”ì¸] {title}")
        
        if news_list:
            print(f"    âœ… {len(news_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return news_list
        
    except Exception as e:
        print(f"    âŒ [Naver Main] ì—ëŸ¬: {e}")
        return []
